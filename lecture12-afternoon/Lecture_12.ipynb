{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuXYLTLs7rlI"
      },
      "source": [
        "# Lecture 12\n",
        "\n",
        "Here we use the E-car dataset on car loan applications.\n",
        "\n",
        "IF you're running this on Google Colab, and only then, should you run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2lWLSol78F8"
      },
      "outputs": [],
      "source": [
        "# !! Run this on Google Colab only.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn01QLMe8HVw"
      },
      "source": [
        "Import the required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WiD9xJk8Jfj"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "import sklearn.neighbors\n",
        "import sklearn.datasets\n",
        "import sklearn.neural_network\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivf6xuLs8N6W"
      },
      "source": [
        "Load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNfDlC6w8PW_"
      },
      "outputs": [],
      "source": [
        "DATA_FILEPATH = \"drive/MyDrive/e_car_data.csv\"\n",
        "df = pd.read_csv(DATA_FILEPATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY2wD2c6-rfz"
      },
      "source": [
        "Show summary statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08A2CnnZ-xYH"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZybA9898hwe"
      },
      "source": [
        "Place loans into 8 unordered classes / bins, depending on their acceptance (0-3 for denied, 4-7 for accepted) and their Annual Percentage Rate (APR, into 4 classes each):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Twh5ek8tTj"
      },
      "outputs": [],
      "source": [
        "def loan_to_bin(accept_value, apr_value):\n",
        "  offset = 0\n",
        "  if accept_value:\n",
        "    offset = 4\n",
        "\n",
        "  if apr_value < 4:\n",
        "    return offset\n",
        "\n",
        "  if apr_value < 6:\n",
        "    return 1 + offset\n",
        "\n",
        "  if apr_value < 8:\n",
        "    return 2 + offset\n",
        "\n",
        "  return 3 + offset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLqgrt7v9KXY"
      },
      "source": [
        "Create new labels for the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QkZwo3V93D-"
      },
      "outputs": [],
      "source": [
        "apr = df['apr'].values\n",
        "accept = df['accept'].values\n",
        "\n",
        "y = []\n",
        "for i in range(len(apr)):\n",
        "    accept_value = accept[i]\n",
        "    apr_value = apr[i]\n",
        "    bin = loan_to_bin(accept_value, apr_value)\n",
        "    y.append(bin)\n",
        "\n",
        "# Alternative in one-line with a list comprehension (but less readable):\n",
        "# y = [loan_to_bin(accept[i], apr[i]) for i in range(len(apr))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Inm5AwEWtp"
      },
      "source": [
        "Define the features and preprocess (or scale, or normalize) them, to help with convergence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX9QjUY4ERb9"
      },
      "outputs": [],
      "source": [
        "columns = ['tier',\n",
        "           'amount',\n",
        "           'apr',\n",
        "           'prime',\n",
        "           'fico',\n",
        "           'competition apr',\n",
        "           'partner bin']\n",
        "x = df[columns].values\n",
        "\n",
        "# Don't forget to scale features!\n",
        "x_scaled = preprocessing.scale(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvb9gygP-bfF"
      },
      "source": [
        "Split the data into three groups: training, validation, and testing (60/20/20 split):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWSReyJHEtk8"
      },
      "outputs": [],
      "source": [
        "# Split into training (60%), validation (20%), testing (20%)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train,\n",
        "                                                  y_train,\n",
        "                                                  test_size=0.25, # 0.25 x 0.8 = 0.2\n",
        "                                                  random_state=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zA25wb5LaMl"
      },
      "source": [
        "# Multi-layer perceptron\n",
        "\n",
        "We'll use a model from SciKit-Learn, which already has all we need (e.g., cross-entropy loss function). We define two hidden layers, with 64 and 32 neurons, and fit it to data. Notice that the accuracy has impoved to around 80%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW7tvU76LjSs"
      },
      "outputs": [],
      "source": [
        "mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(64, 32),\n",
        "                                           activation=\"logistic\",\n",
        "                                           max_iter=1000,\n",
        "                                           random_state=42)\n",
        "\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = mlp.predict(x_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of multi-layer perceptron on test data: %.5f\" %\n",
        "      accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq8BLHzqipyZ"
      },
      "source": [
        "# Accuracy comparison of our 3 classifiers\n",
        "\n",
        "Let's compare this to the other two classifiers we have: k-Nearest Neighbors and multinomial logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ9G6P-HipQU"
      },
      "outputs": [],
      "source": [
        "logistic = LogisticRegression(max_iter=int(1e5))\n",
        "logistic.fit(x_train, y_train)\n",
        "y_logistic_pred = logistic.predict(x_test)\n",
        "accuracy_logistic = metrics.accuracy_score(y_test, y_logistic_pred)\n",
        "print(\"Accuracy of logistic regression on test data: %.5f\" %\n",
        "      accuracy_logistic)\n",
        "\n",
        "knn = sklearn.neighbors.KNeighborsClassifier()\n",
        "knn.fit(x_train, y_train)\n",
        "y_knn_pred = knn.predict(x_test)\n",
        "accuracy_knn = metrics.accuracy_score(y_test, y_knn_pred)\n",
        "print(\"Accuracy of kNN regression on test data: %.5f\" %\n",
        "      accuracy_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrpAMXiutMOI"
      },
      "source": [
        "# How to choose the number of neighbors / hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSwsta7nteMT"
      },
      "outputs": [],
      "source": [
        "# Define a helper function to be DRY.\n",
        "def fit_knn(k, x_train, y_train, x_test, y_test):\n",
        "  knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(x_train, y_train)\n",
        "  y_knn_pred = knn.predict(x_test)\n",
        "  accuracy = metrics.accuracy_score(y_test, y_knn_pred)\n",
        "  return knn, accuracy\n",
        "\n",
        "\n",
        "# Find the best k.\n",
        "k_range = range(1, 100)\n",
        "accuracy_range = []\n",
        "for k in k_range:\n",
        "  _, accuracy = fit_knn(k, x_train, y_train, x_val, y_val)\n",
        "  accuracy_range.append(accuracy)\n",
        "\n",
        "plt.plot(k_range, accuracy_range)\n",
        "\n",
        "best_k = k_range[np.argmax(accuracy_range)]\n",
        "\n",
        "# Compute accuracy with the best k.\n",
        "_, acc = fit_knn(best_k, x_train, y_train, x_test, y_test)\n",
        "print(\"Accuracy of optimized k-nearest neighbors (k=%d): %.3f\" % (best_k, acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pozxYS9fKDf4"
      },
      "source": [
        "# Question\n",
        "\n",
        "Why is the kNN accuracy smaller with k = 13 then before, with default value of k? That is: why did the optimization of hyper-parameters not optimize the accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHqpPJ_3mdv6"
      },
      "source": [
        "# Revisiting MNIST hand-written digits\n",
        "\n",
        "In lecture 1 and assignment 2, we used 1-nearest neighbor for MNIST. Here we code it again, using SciKit-Learn, and compare its accuracy to the a neural network. In case you don't have Tensorflow installed, you can use this cell (with a smaller version of the dataset):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma-D27Jim9xZ"
      },
      "outputs": [],
      "source": [
        "mnist = sklearn.datasets.load_digits()\n",
        "print(\"Data shape:\", mnist.data.shape)\n",
        "x = mnist.data\n",
        "y = mnist.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,\n",
        "                                                    y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgzoP2n-qaGm"
      },
      "source": [
        "If you have Tensorflow / Keras installed, you can use the full dataset with 60k images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyCxHUqHrBZ6"
      },
      "outputs": [],
      "source": [
        "#!pip install keras\n",
        "import keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "print(\"Shape before flattening:\", x_train.shape)\n",
        "\n",
        "# Flatten the data, for input into the Multi-Layer Perceptron.\n",
        "# If we used Tensorflow, we could use a tf.keras.layers.Flatten layer\n",
        "# to convert the image.\n",
        "\n",
        "x_train = x_train.reshape([x_train.shape[0], -1])\n",
        "x_test = x_test.reshape([x_test.shape[0], -1])\n",
        "print(\"Shape after flattening:\", x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pGQ0V23qTgs"
      },
      "source": [
        "Check the output counts, and split the data into training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDWFGoQ2qSOE"
      },
      "outputs": [],
      "source": [
        "print(\"Target classes: \", np.unique(y_train, return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOaYwpj2m_LN"
      },
      "outputs": [],
      "source": [
        "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(x_train, y_train)\n",
        "y_knn_pred = knn.predict(x_test)\n",
        "accuracy_knn = metrics.accuracy_score(y_test, y_knn_pred)\n",
        "print(\"Accuracy of kNN regression on test data: %.5f\" %\n",
        "      accuracy_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: estimate a neural network on MNIST data\n",
        "\n",
        "Complete this next cell."
      ],
      "metadata": {
        "id": "9PpuWQAyDpK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yWNoxZys3eg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# TODO: train a neural network to recognize handwritten digits, and gauge the\n",
        "# accuracy of the model\n",
        "\n",
        "\n",
        "\n",
        "# This line prints how long it took to train the model and estimate accuracy.\n",
        "minutes = (time.time() - start) / 60\n",
        "print(\"Elapsed: %.1f minutes\" % minutes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}